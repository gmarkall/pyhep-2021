{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b4a6e6",
   "metadata": {},
   "source": [
    "# Numba's CUDA Functionality (at PyHEP 2021)\n",
    "![Numba and CUDA logos](title-image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72166da9",
   "metadata": {},
   "source": [
    "## Hello!\n",
    "\n",
    "<img style=\"float: right;\" src=\"gmarkall-small.jpg\">\n",
    "\n",
    "* Graham Markall, Software Engineer, NVIDIA\n",
    "  - <gmarkall@nvidia.com>\n",
    "  - [@gmarkall on Twitter](https://twitter.com/gmarkall)\n",
    "  - [@gmarkall on Github](https://github.com/gmarkall)\n",
    "* Joined  NVIDIA Dec 2019. Anaconda 2014 - 2016\n",
    "* Talk to me about Numba and CUDA!\n",
    "  - (or RISC-V, numerical methods, open source hardware, cycling, running)\n",
    "  \n",
    "## Talk plan\n",
    "\n",
    "- 15 minute overview of Numba + CUDA\n",
    "- 5 minute demo\n",
    "- 10 minute Q&A\n",
    "\n",
    "A broad, low-technical overview to understand:\n",
    "\n",
    "- What is Numba?\n",
    "- Who uses Numba?\n",
    "- Should you use it (with CUDA)?\n",
    "- How do you use it (with CUDA)?\n",
    "- How to get access to CUDA hardware\n",
    "\n",
    "## What is Numba?\n",
    "\n",
    "- A compiler for Python, aimed at array-oriented numerical code\n",
    "  - use when you want better performance out of Python\n",
    "\n",
    "- (Possible) Alternatives:\n",
    "  - Native libraries with Python bindings\n",
    "  - Cython\n",
    "  - Write \"native\" code (C / Fortran / CUDA, etc.) and call from Python\n",
    "  - PyPy?\n",
    "  - Julia?\n",
    "- Compiles a subset of Python\n",
    "  - supports a large subset of the Python language and NumPy functionality\n",
    "  - *opt-in*, using the `@jit` decorator: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e0d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from matplotlib.pylab import imshow, show\n",
    "from time import perf_counter\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "max_iters = 20\n",
    "min_x, max_x, min_y, max_y = (-2.0, 1.0, -1.0, 1.0)\n",
    "image_size = (1000, 1500)\n",
    "\n",
    "\n",
    "# Mandelbrot functions\n",
    "\n",
    "@jit\n",
    "def mandel(x, y):\n",
    "    c = complex(x,y)\n",
    "    z = 0.0j\n",
    "    for i in range(max_iters):\n",
    "        z = z*z + c\n",
    "        if (z.real * z.real + z.imag * z.imag) >= 4:\n",
    "            return i\n",
    "\n",
    "    return 255\n",
    "\n",
    "@jit\n",
    "def create_fractal():\n",
    "    image = np.zeros(image_size, dtype=np.uint8)\n",
    "    \n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "\n",
    "    pixel_size_x = (max_x - min_x) / width\n",
    "    pixel_size_y = (max_y - min_y) / height\n",
    "    \n",
    "    for x in range(width):\n",
    "        real = min_x + x * pixel_size_x\n",
    "        for y in range(height):\n",
    "            imag = min_y + y * pixel_size_y\n",
    "            color = mandel(real, imag)\n",
    "            image[y, x] = color\n",
    "    \n",
    "    return image\n",
    "\n",
    "            \n",
    "# Call JITted function\n",
    "\n",
    "start = perf_counter()\n",
    "image = create_fractal()\n",
    "end = perf_counter()\n",
    "\n",
    "print(f\"create_fractal() execution time: {end - start} seconds\")\n",
    "\n",
    "imshow(image)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7418645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call JITted function\n",
    "start = perf_counter()\n",
    "create_fractal()\n",
    "end = perf_counter()\n",
    "\n",
    "print(f\"create_fractal() execution time: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ced99",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "create_fractal.py_func()\n",
    "end = perf_counter()\n",
    "\n",
    "print(f\"create_fractal() Python-only execution time: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b8f740",
   "metadata": {},
   "source": [
    "* Approx 10x speedup in this case.\n",
    "* Typical speedups around 10x - 1/200x over Pure Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b845f5",
   "metadata": {},
   "source": [
    "## What just happened?\n",
    "\n",
    "Numba's pipeline:\n",
    "\n",
    "![Numba pipeline diagram](numba-pipeline.svg)\n",
    "\n",
    "The frontend / backend split:\n",
    "\n",
    "<img alt=\"Numba architecture diagram\" src=\"numbacompiler.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841a7f17",
   "metadata": {},
   "source": [
    "## Who uses Numba?\n",
    "\n",
    "![Github Numba stats](github-numba-stats.png)\n",
    "\n",
    "* PyPI 50,000 / conda 15,000 downloads per day\n",
    "\n",
    "Random sample of applications using it for CUDA:\n",
    "\n",
    "* [Poliastro](https://github.com/poliastro/poliastro) (astrodynamics)\n",
    "* [FBPIC](https://fbpic.github.io/) (CUDA-accelerated plasma physics)\n",
    "* [UMAP](https://umap-learn.readthedocs.io/en/latest/) (manifold learning)\n",
    "* [RAPIDS](https://rapids.ai/) (data science / machine learning)\n",
    "* [Talks on more applications](https://numba.pydata.org/numba-doc/dev/user/talks.html#talks-on-applications-of-numba) in Numba docs\n",
    "\n",
    "Numba tends to be a building block of software stacks rather than the top layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f2fbd",
   "metadata": {},
   "source": [
    "## Why use Numba?\n",
    "\n",
    "* Stay in your *comfort zone*\n",
    "  - keep all code in Python\n",
    "  - get better performance\n",
    "* Focus on algorithm development\n",
    "  - Minimise development time, use the same test frameworks, etc.\n",
    "  - Maintain interoperability of your code\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8572b69",
   "metadata": {},
   "source": [
    "## Should you use it?\n",
    "\n",
    "Things that work well:\n",
    "\n",
    "* Numerical code, loops\n",
    "* Large amounts of data\n",
    "* Data-parallel operations\n",
    "\n",
    "Things that can be tricky to optimize (**red flags**), particularly on CUDA:\n",
    "\n",
    "* Code using lots of strings or dicts\n",
    "* Inherently serial logic\n",
    "* Calling lots of already-compiled code\n",
    "* Code with a lot of object-oriented patterns and features\n",
    "* Code that's already been heavily optimized using another tool / paradigm (e.g. Cython)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281b1d1",
   "metadata": {},
   "source": [
    "## What is CUDA?\n",
    "\n",
    "### Hardware: NVIDIA GPUs.\n",
    "\n",
    "A separate device, usually attached to the PCIe bus, with its own memory\n",
    "\n",
    "![GPU Address Space Diagram](gpu_memory.png)\n",
    "\n",
    "Many cores, all working in parallel on the same function:\n",
    "\n",
    "<img src=\"gpu-arch.png\" alt=\"GPU Architecture Diagram\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23569f3",
   "metadata": {},
   "source": [
    "### Software\n",
    "\n",
    "Many threads run on many cores, all executing the same function:\n",
    "\n",
    "![Grid of thread blocks](grid-of-thread-blocks.png)\n",
    "\n",
    "... more about this later in the demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0666a",
   "metadata": {},
   "source": [
    "## Getting access to CUDA hardware\n",
    "\n",
    "For the PyHEP audience, options could be:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6416f5e",
   "metadata": {},
   "source": [
    "### Google Colab\n",
    "\n",
    "https://colab.research.google.com: Select **Runtime**, select **Change runtime type**, select **Hardware accelerator** and set to **GPU**\n",
    "\n",
    "- Pros:\n",
    "  - Free\n",
    "  - Uses Jupyter Notebooks\n",
    "- Cons:\n",
    "  - Limited runtime, customizability, old Numba version (0.51.2)\n",
    "  - Availability of GPUs can be limited\n",
    "  - Uses Jupyter Notebooks\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b492aa5",
   "metadata": {},
   "source": [
    "### GPU in your own machine\n",
    "\n",
    "Pros:\n",
    "\n",
    "- Consistent availability\n",
    "- Can install / run anything on your own machine\n",
    "- Can use the debugger (cuda-gdb)\n",
    "\n",
    "Cons:\n",
    "\n",
    "- May be lower performance (especially for 64-bit floating point) than pro-grade hardware like T4, RTX x000\n",
    "- Have to maintain it yourself\n",
    "- Availability a little difficult for high-end GPUs at the moment\n",
    "\n",
    "Recommendation for getting started / development:\n",
    "\n",
    "- GTX 10x0: Old enough to be available at reasonable cost, most Numba CUDA features supported by it\n",
    "  - GTX 1050 needs no extra power connector, just a PCIe x16 slot\n",
    "- Good enough for development / debugging\n",
    "- Move code to higher-performance system later\n",
    "\n",
    "<img alt=\"GTX 1050 Ti Founders edition GPU\" src=\"gtx-1050-ti.webp\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ff9f3",
   "metadata": {},
   "source": [
    "### An HPC system you have access to...?\n",
    "\n",
    "Pros:\n",
    "\n",
    "- You may already have access to it\n",
    "- Access to a lot of GPUs / nodes\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Hard to work interactively\n",
    "- The usual batch job submission cons..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c924ba6",
   "metadata": {},
   "source": [
    "### Cloud services\n",
    "\n",
    "* AWS\n",
    "* Azure\n",
    "* Google\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f8422e",
   "metadata": {},
   "source": [
    "# Demo: Diffusion code example\n",
    "\n",
    "This example provides a short illustration of how to use Numba with the CPU parallel and CUDA targets.\n",
    "\n",
    "The example function is very simple:\n",
    "* We begin with a Gaussian hump of tracer.\n",
    "* A finite-difference kernel diffuses the tracer over many iterations.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ca72e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, njit, prange\n",
    "import numba\n",
    "import numpy as np\n",
    "import pylab\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numba.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7fff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1142815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/proc/cpuinfo', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d5f2e",
   "metadata": {},
   "source": [
    "### Run parameters\n",
    "\n",
    "These are chosen for relatively quick execution, but with large enough data to make efficient use of CUDA.\n",
    "* 10,000 iterations.\n",
    "* A grid of 1,000 by 1,000 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb3c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 20000\n",
    "POINTS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30142811",
   "metadata": {},
   "source": [
    "### Generation of initial conditions\n",
    "\n",
    "* Creates a hump of tracer.\n",
    "* We use Numba's `@njit` decorator to speed this up\n",
    "  - Compiles the Python to native code.\n",
    "  - Single-threaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ccef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def gauss2d(x, y): \n",
    "    grid = np.empty_like(x)\n",
    "\n",
    "    a = 1.0 / np.sqrt(2 * np.pi)\n",
    "\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            grid[i, j] = a * np.exp(-(x[i, j]**2 / 2 + y[i, j]**2\n",
    "                                      / 2)) \n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "X = np.linspace(-5, 5, POINTS)\n",
    "Y = np.linspace(-5, 5, POINTS)\n",
    "x, y = np.meshgrid(X, Y)\n",
    "\n",
    "z = gauss2d(x, y)\n",
    "\n",
    "pylab.imshow(z)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4e3f6",
   "metadata": {},
   "source": [
    "## Python version\n",
    "\n",
    "We don't actually run this, because it will take too long.\n",
    "\n",
    "Having the function definition here illustrates our starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d763e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x0, x1):\n",
    "    for i in range(1, x0.shape[0] - 1): \n",
    "        for j in range(1, x0.shape[1] - 1): \n",
    "            x1[i, j] = 0.25 * (x0[i, j - 1] + x0[i, j + 1] +\n",
    "                               x0[i - 1, j] + x0[i + 1, j]) \n",
    "\n",
    "def run_python():\n",
    "    z0 = z.copy()\n",
    "    z1 = np.zeros_like(z0)\n",
    "    \n",
    "    start = perf_counter()\n",
    "\n",
    "    for i in range(ITERATIONS):\n",
    "        if (i % 2) == 0:\n",
    "            smooth(z0, z1)\n",
    "        else:\n",
    "            smooth(z1, z0)\n",
    "\n",
    "    end = perf_counter()\n",
    "\n",
    "    time_python = end - start\n",
    "    return z0, time_python\n",
    "\n",
    "\n",
    "# Not running because it will take too long!\n",
    "# z_python, time_python = run_python()\n",
    "# pylab.imshow(z_python)\n",
    "# pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceaa67f",
   "metadata": {},
   "source": [
    "## CPU JIT\n",
    "\n",
    "We parallelise execution on the CPU, for better performance:\n",
    "* Passing `parallel=True` to the `@njit` decorator\n",
    "* Using a `prange` instead of a `range` to indicate how to parallelize.\n",
    "\n",
    "Benchmarking the CPU implementation requires a little care:\n",
    "\n",
    "* The first call to the `smooth_jit()` function will trigger JIT compilation of the function which is quite costly in comparison to an iteration (a few hundred milliseconds, perhaps)\n",
    "  * So, we make a \"dummy call\" before starting our timed loop, so the JIT compilation is out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def smooth_jit(x0, x1):\n",
    "    for i in prange(1, x0.shape[0] - 1): \n",
    "        for j in range(1, x0.shape[1] - 1): \n",
    "            x1[i, j] = 0.25 * (x0[i, j - 1] + x0[i, j + 1] +\n",
    "                               x0[i - 1, j] + x0[i + 1, j]) \n",
    "\n",
    "\n",
    "def run_cpu_jit():\n",
    "    z0 = z.copy()\n",
    "    z1 = np.zeros_like(z0)\n",
    "\n",
    "    # Warm up JIT\n",
    "    smooth_jit(z0, z1) \n",
    "\n",
    "    start = perf_counter()\n",
    "\n",
    "    for i in range(ITERATIONS):\n",
    "        if (i % 2) == 0:\n",
    "            smooth_jit(z0, z1) \n",
    "        else:\n",
    "            smooth_jit(z1, z0) \n",
    "\n",
    "    end = perf_counter()\n",
    "\n",
    "\n",
    "    time_cpu = end - start\n",
    "    return z0, time_cpu\n",
    "\n",
    "z_cpu, time_cpu = run_cpu_jit()\n",
    "\n",
    "pylab.imshow(z_cpu)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10841da2",
   "metadata": {},
   "source": [
    "## CUDA JIT\n",
    "\n",
    "Parallel implementation on the CPU requires some more changes:\n",
    "\n",
    "* Use of the `@cuda.jit` decorator.\n",
    "* The `for` loops are \"flattended\" - `smooth_cuda()` is invoked by many threads.\n",
    "  - Each thread gets its index `(i, j)` from `cuda.grid(2)` (for a 2D grid).\n",
    "  - We check this kernel's indices are in the bounds\n",
    "    - (the interior of the domain)\n",
    "* The number of threads to launch (the grid and block dimensions) are computed.\n",
    "  - We use a fixed size block here, and work out how many we need based on the input size.\n",
    "  - We could also use a loop inside the `smooth_cuda` function (a *grid-stride loop*) but don't here for simplicity.\n",
    "  \n",
    "Benchmarking the CUDA implementation needs a little more care:\n",
    "\n",
    "* We copy data over to the device first.\n",
    "  * If we don't do this, we end up creating implicit copies every time the kernel is launched.\n",
    "  * Implicit copies insert a lot of synchronization points and slow down execution considerably.\n",
    "* We call `cuda.synchronize()` before ending timing.\n",
    "  * Kernel launches are asynchronous, so if we don't do this the GPU still can have work to do when we stop the timer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a1d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def smooth_cuda(x0, x1):\n",
    "    i, j = cuda.grid(2)\n",
    "\n",
    "    i_in_bounds = (i > 0) and (i < (x0.shape[0] - 1))\n",
    "    j_in_bounds = (j > 0) and (j < (x0.shape[1] - 1))\n",
    "\n",
    "    if i_in_bounds and j_in_bounds:\n",
    "        x1[i, j] = 0.25 * (x0[i, j - 1] + x0[i, j + 1] +\n",
    "                           x0[i - 1, j] + x0[i + 1, j])\n",
    "\n",
    "\n",
    "def run_cuda_jit():\n",
    "    # Copy to device\n",
    "    z0 = cuda.to_device(z)\n",
    "    z1 = cuda.device_array_like(np.zeros_like(z))\n",
    "\n",
    "    # Warm up JIT\n",
    "    blockdim = (16, 16)\n",
    "    griddim = ((z0.shape[0] // blockdim[0]) + 1, (z0.shape[1] // blockdim[1]) + 1)\n",
    "    smooth_cuda[griddim, blockdim](z0, z1)\n",
    "\n",
    "    start = perf_counter()\n",
    "\n",
    "    for i in range(ITERATIONS):\n",
    "        if (i % 2) == 0:\n",
    "            smooth_cuda[griddim, blockdim](z0, z1)\n",
    "        else:\n",
    "            smooth_cuda[griddim, blockdim](z1, z0)\n",
    "\n",
    "    # Make sure the GPU is finished before we stop timing\n",
    "    cuda.synchronize()\n",
    "    end = perf_counter()\n",
    "\n",
    "    \n",
    "    time_cuda = end - start\n",
    "    return z0.copy_to_host(), time_cuda\n",
    "\n",
    "z_cuda, time_cuda = run_cuda_jit()\n",
    "\n",
    "pylab.imshow(z_cuda)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f2feb1",
   "metadata": {},
   "source": [
    "## Check and compare results\n",
    "\n",
    "Hopefully the difference between the solutions (CPU and CUDA) should look fairly small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc0985",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.abs(z_cpu - z_cuda)\n",
    "pylab.imshow(diff)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa2988d",
   "metadata": {},
   "source": [
    "## Compare performance\n",
    "\n",
    "Before giving any performance measurements, it's always a good idea to (somewhat) rigorously check correctness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14298f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the solutions are reasonably close\n",
    "np.testing.assert_allclose(z_cpu, z_cuda)\n",
    "\n",
    "# If so, print out execution times\n",
    "print(f\"CPU time: {time_cpu:2.2f} seconds\")\n",
    "print(f\"CUDA time: {time_cuda:2.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427353d8",
   "metadata": {},
   "source": [
    "# Further resources / next steps\n",
    "\n",
    "Learning how to use Numba:\n",
    "\n",
    "* A recent and comprehensive course on using Numba with CUDA: https://github.com/numba/nvidia-cuda-tutorial\n",
    "* A 5-minute guide to Numba in the Numba docs: https://numba.readthedocs.io/en/latest/user/5minguide.html\n",
    "\n",
    "Learning more about Numba:\n",
    "* The life of a Numba kernel: https://medium.com/rapids-ai/the-life-of-a-numba-kernel-a-compilation-pipeline-taking-user-defined-functions-in-python-to-cuda-71cc39b77625\n",
    "* Numba documentation: https://numba.readthedocs.io/en/latest/\n",
    "\n",
    "Learning about CUDA and GPUs in general:\n",
    "\n",
    "* CUDA C Programming Guide: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\n",
    "* An even easier introduction to CUDA: https://developer.nvidia.com/blog/even-easier-introduction-cuda/\n",
    "\n",
    "Discussion / help:\n",
    "\n",
    "- [Numba Gitter channel](https://gitter.im/numba/numba) - good for quick questions and interactive discussion\n",
    "- [Numba Discourse](https://numba.discourse.group) - better for more in-depth questions\n",
    "- [Numba Issue tracker](https://github.com/numba/numba/issues) - better for reporting bugs and making feature requests - not usually the first port of call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c7035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
